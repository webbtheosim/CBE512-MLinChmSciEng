{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Lw7tcolDvH"
      },
      "source": [
        "## **Name**:\n",
        "## **Status**: (e.g., UG3, G1, etc.)\n",
        "## **Department**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wr-KFV4plFjv"
      },
      "source": [
        "# ***CBE 512. Machine Learning in Chemical Science and Engineering.***\n",
        "## **Assignment 04:** *Neural Networks*\n",
        "### &#169; Princeton University\n",
        "### **DUE**: 11:59pm, October 26, 2025\n",
        "### **Notes**: Supply responses/solutions in a packaged format, including notebook (.ipynb), .docx, or .pdf for your text, and .pdf for slides. Your assignment should be submitted to Canvas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILuWZs8lzF7"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## **Problem 1 (100 points):**\n",
        "\n",
        "Examine the contents of `concrete_data.csv` and the description in `readme.concrete_data.csv`.\n",
        "You can download and view it on your own or easily examine the contents on github [here](https://raw.githubusercontent.com/webbtheosim/CBE512-MLinChmSciEng/main/data/concrete_data.csv) and [here](https://github.com/webbtheosim/CBE512-MLinChmSciEng/blob/main/data/readme.concrete_data.txt).\n",
        "\n",
        "This is a dataset featuring 1030 experimental measurements that characterize the compressive strength of concrete that is distinguished by eight material descriptors: 'Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer', 'Coarse Aggregate', 'Fine Aggregate', and 'Age'. The first seven descriptors reflect concentrations of components (kg per cubic meter of mixture) and the last is self-explanatory (units of days). All the data is \"raw,\" so you should conduct your own preprocessing.\n",
        "\n",
        "Overall, you are going to build and test neural networks to predict compressive strength of concrete from the materials descriptors. *For computational expediency, you may use simple train-test splits* in lieu of k-fold cross-validation for reporting.\n",
        "\n",
        "If you are having trouble accomplishing the tasks with the amount of data provided, you can truncate the dataset as needed (e.g., use 200 points instead of 1000).\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HseRYsXZnzw6"
      },
      "source": [
        "**(a)** Evaluate how training dataset size affects neural network performance using a dataset with over 1000 data points. Start by splitting the data into 80% training and 20% testing. **Keep the test set fixed for consistent evaluation**. From the training set, randomly sample different portions (e.g., 20%, 40%, 60%, etc.). For each subset, train a neural network using the architecture [200->100->50->10->1] with ReLU activations in all layers except the final one. Use the Adam optimizer, MSE loss, 500 epochs, and a batch size of 16, but feel free to adjust these.\n",
        "\n",
        "After training, evaluate each model on the fixed test set. Plot test-set performance against training set size and analyze whether all data points are needed for effective results.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aef_EncKnzw7"
      },
      "source": [
        "**Student's Response:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W96xR_tFnzw7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vqmhVqgmone"
      },
      "source": [
        "**(b)** Technically, the number of epochs for training is an important hyperparameter for model construction. We have often mentioned the approach of \"Early Stopping,\" for which training stops (or returns) back to the state that minimized the validation loss (usually to mitigate subsequent overfitting).\n",
        "\n",
        "First, make a plot of the training and test set performance versus the number of epochs using the model that used 60% of the total available data for training. Is there a number of epochs for training for which the test loss reaches a minimum? Note that you can specify data for \"validation\" within the Keras API (rather than specifying the split ratio).\n",
        "\n",
        "Next, use Keras's callback functionality to implement `EarlyStopping` and save the model as a .hdf5 format using `ModelCheckpoint`. For early stopping, you want to monitor `val_loss`, with `mode=min` and `patience=50`. Report what epoch yielded your best model. Should this be the same as what you saw in the previous run manual run?\n",
        "\n",
        "Finally, use `load_model` to load in your saved best model, and report the MSE/R2 performance of your best early stopping model on the train/test set.\n",
        "\n",
        "Early stopping implementation is described [here](https://keras.io/api/callbacks/early_stopping/). Model checkpoint [here](https://keras.io/api/callbacks/model_checkpoint/).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89kw8p7Ynzw8"
      },
      "source": [
        "**Student's Response:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpXxm4j9nzw8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJgBlEv5nzw9"
      },
      "source": [
        "**(c)** We have not talked significantly about weight initialization for neural networks. During model optimization, we are trying identify the weights that minimize our target loss -- this process is guided through backpropogation. Like most optimizations, success can depend on initial guesses. With neural networks, the process is confounded by the so-called vanishing/exploding gradient problem.\n",
        "\n",
        "Early work in weight initialization helped mitigate that problem by being smarter about initialization.\n",
        "Let's explore a little bit about how our weight initialization choices can influence model convergence. Make a bar plot of the train/test performance of your neural network model using three settings for the weight initialization: [\"zeros\", \"RandomUniform\", \"GlorotUniform\",\"HeNormal\"]. Comment on any tangible observations.\n",
        "\n",
        "The initializers are described [here](https://keras.io/api/layers/initializers/).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnSQitrpnzw9"
      },
      "source": [
        "**Student's Response:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEknWvOknzw-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZlMZJ5Dnzw-"
      },
      "source": [
        "**(d)** You have been working with a simple feedforward neural network using Keras’s `Sequential` model. Now, modify your network architecture using Keras’s `Model` functional API to introduce branching and layer concatenation. For example, implement a skip (residual) connection, where the input from an earlier layer bypasses intermediate layers and is added to the output of a later layer. This architecture can help mitigate the vanishing gradient problem and improve training, especially in deeper networks.\n",
        "\n",
        "Train the modified model using the same settings (Adam optimizer, MSE loss, 500 epochs), with 60% of the total data for training, and use `EarlyStopping` with `ModelCheckpoint`. Compare the performance of this model on the test set to your original model's performance.\n",
        "\n",
        "\n",
        "The Model functional API is described [here](https://keras.io/guides/functional_api/).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfPlQ7ypnzw-"
      },
      "source": [
        "**Student's Response:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0-nsJ_xpEsI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_DLDWkEnzw_"
      },
      "source": [
        "**(e)** Dropout is a regularization technique used to reduce overfitting in neural networks by randomly deactivating units during training. Modify your existing model by adding `Dropout` layers after each Dense layer (except the output layer) with dropout rates of 0.2 and 0.5. Train the modified model using the same settings (Adam optimizer, MSE loss, 500 epochs) and architecture as in part **b**, with 60% of the total data for training, and use `EarlyStopping` with `ModelCheckpoint`.\n",
        "\n",
        "Compare the test set performance with the baseline model (no Dropout). How does Dropout affect generalization, and what dropout rate yields the best results?\n",
        "\n",
        "The Dropout layer is described [here](https://keras.io/api/layers/regularization_layers/dropout/).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr1IKbR4nzw_"
      },
      "source": [
        "**Student's Response:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKgGhvx1nzw_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_H05YKbnzw_"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}